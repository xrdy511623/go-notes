
---
TCP拥塞控制是如何导致业务性能抖动的？
---

# 1 TCP拥塞控制的原理





![cwnd-phase.png](images%2Fcwnd-phase.png)





拥塞控制主要是以下四个算法：
慢启动
拥塞避免
拥塞发生
快速恢复


## 1.1 慢启动
TCP 在刚建⽴连接完成后，⾸先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果⼀上来就发⼤量的数据，
这不是给⽹网络添堵吗?
慢启动的算法记住一个规则就行:当发送⽅每收到⼀个 ACK，拥塞窗⼝ cwnd 的⼤小就会加 1。 这里假定拥塞窗口 cwnd 和发送窗口 swnd 相等，
下⾯举个例子:
连接建立完成后，⼀开始初始化 cwnd = 1 ，表示可以传⼀一个 MSS ⼤小的数据。
当收到⼀个 ACK 确认应答后，cwnd 增加 1，于是⼀次能够发送 2 个
当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以⽐之前多发2 个，所以这⼀次能够发送 4个
当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以⽐之前多发 4 个，所以这⼀次能够发送 8 个。





![slow-start-up.png](images%2Fslow-start-up.png)





可以看出慢启动算法，发包的个数是指数性的增长。 那慢启动涨到什么时候是个头呢?
有⼀个叫慢启动⻔门限 ssthresh (slow start threshold)状态变量量。 当 cwnd < ssthresh 时，使⽤用慢启动算法。
当 cwnd >= ssthresh 时，就会使⽤用「拥塞避免算法」。


## 1.2 拥塞避免算法
⼀般来说 ssthresh 的⼤小是 65535 字节。 那么进入拥塞避免算法后，它的规则是:每当收到⼀一个 ACK 时，cwnd 增加 1/cwnd。 
接上前⾯的慢启动的例子，现假定 ssthresh 为 8 :
当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8个 ACK 确认 cwnd 一共增加 1，于是这⼀次 能够发送 9 个 MSS ⼤小的数据，变成了了线性增长。





![avoid-crowd.png](images%2Favoid-crowd.png)





所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增⻓变成了线性增⻓，还处于增⻓阶段，但是增⻓速度缓慢了一些。
就这么⼀直增长后，⽹络就会慢慢进⼊拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进⾏重传。
当触发了重传机制，也就进⼊了「拥塞发生算法」。

## 1.3 拥塞发生
当⽹络出现拥塞，也就是发生数据包重传，重传机制主要有两种:
> 超时重传 
> 快速重传

### 1.3.1 超时重传
当发⽣了超时重传，就会使⽤拥塞发生算法。此时，ssthresh 和 cwnd 的值会发生变化:
ssthresh 设为 cwnd/2 ， cwnd 重置为 1





![timeout-retransmission.png](images%2Ftimeout-retransmission.png)'






接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是⼀旦「超时重传」，⻢上回到解放前。但是这种⽅式太激进了，
反应也很强烈，会造成⽹络卡顿。

### 1.3.2 快速重传
还有更好的⽅式，那就是「快速重传算法」。当接收方发现丢了⼀个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速
地重传，不必等待超时再重传。
TCP 认为这种情况不严重，因为大部分没丢，只丢了一⼩部分，此时 ssthresh 和 cwnd 变化如下:
cwnd = cwnd/2, 也就是设置为原来的一半;
ssthresh = cwnd;
进入快速恢复算法

## 1.4 快速恢复
快速恢复
快速重传和快速恢复算法⼀般同时使⽤，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明⽹络也不那么糟糕，所以没有必要像
RTO 超时那么强烈。
正如前面所说，进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了:
cwnd = cwnd/2, 也就是设置为原来的一半;
ssthresh = cwnd;

然后，进入快速恢复算法如下:
拥塞窗⼝ cwnd = ssthresh + 3 ( 3 的意思是确认有 3 个数据包被收到了); 
重传丢失的数据包;
如果再收到重复的 ACK，那么 cwnd 增加 1;
如果收到新数据的 ACK 后，把 cwnd 设置为第⼀步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从
duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也就是再次进入拥塞避免状态;





![fast-recover.png](images%2Ffast-recover.png)





也就是没有像「超时重传」⼀夜回到解放前，⽽是还在⽐较⾼的值，后续呈线性增长。


# 2 如何针对拥塞控制进行性能调优

## 2.1 init_cwnd 调整初始拥塞窗口大小 
初始发送数据包的数量是由 init_cwnd（初始拥塞窗口）来决定的，该值在 Linux 内核中被设置为 10（TCP_INIT_CWND），这是由 Google 
的研究人员总结出的一个经验值，这个经验值也被写入了RFC6928。并且，Linux 内核在 2.6.38 版本中也将它从默认值 3 修改为了 Google 建议的 10。

增大 init_cwnd 可以显著地提升网络性能，因为这样在初始阶段就可以一次性发送很多TCP Segments。

如果你的内核版本比较老（低于 CentOS-6 的内核版本），那不妨考虑增加 init_cwnd 到 10。如果你想要把它增加到一个更大的值，也不是不可以，
但是你需要根据你的网络状况多做一些实验，从而得到一个较为理想的值。因为如果初始拥塞窗口设置得过大的话，可能会引起很高的 TCP 重传率。当然，
你也可以通过 ip route 的方式来更加灵活地调整该值，甚至将它配置为一个 sysctl 控制项。

增大 init_cwnd 的值对于提升短连接的网络性能会很有效，特别是数据量在慢启动阶段就能发送完的短连接，比如针对 http 这种服务，http 的短连接
请求数据量一般不大，通常在慢启动阶段就能传输完，这些都可以通过 tcpdump 来进行观察。

## 2.2 设置RTO超时时间，避免应用在发送数据包时阻塞太久

```shell
ret = setsockopt(sockfd, SOL_SOCKET, SO_SNDTIMEO, &timeout, len);
```
当业务发现该 TCP 连接超时后，就会主动断开该连接，然后尝试去使用其他的连接。

将 TCP RTO min、TCP RTO max、 TCP RTO init 更改为可以使用 sysctl 来灵活控制的变量，从而根据实际情况来做调整，比
如说针对数据中心内部的服务器，我们可以适当地调小这几个值，从而减少业务阻塞时间。也就是说，设置全局的 RTO 时间
（设置一次，所有的 TCP 连接都能生效）


总体来说，拥塞控制就是根据 TCP 的数据传输状况来灵活地调整拥塞窗口，从而控制发送方发送数据包的行为。换句话说，拥塞窗口的
大小可以表示网络传输链路的拥塞情况。TCP 连接 cwnd 的大小可以通过 ss 这个命令来查看：

```shell
ss -tin

State      Recv-Q Send-Q     Local Address:Port          Peer Address:Port
ESTAB      0      0          192.168.1.10:22             203.0.113.5:34567
    cubic wscale:7,7 rto:200 rtt:15.4/4.6 ato:40 mss:1448 cwnd:10 ssthresh:8
```


## 2.3 通过tcp_probe内核模块监控TCP拥塞窗口的变化
Linux 内核中的 tcp_probe 模块可以帮助监控 TCP 拥塞窗口的变化，tcp_bbr 模块也可以提供有关流控的详细信息。以下是使用 tcp_probe 的步骤：

> 加载 tcp_probe 模块：
```shell
sudo modprobe tcp_probe port=all full=1
```

> 查看 tcp_probe 输出（cwnd 包含在结果中）：
```shell
sudo cat /sys/kernel/debug/tracing/trace_pipe
```
这个输出会实时显示当前系统中 TCP 连接的拥塞窗口大小（cwnd），以及其他与 TCP 传输相关的参数。


# 3 如何监控接收窗口太小导致无法收包的情况?





![cwnd-and-rwnd.png](images%2Fcwnd-and-rwnd.png)





如上图所示，接收方在收到数据包后，会给发送方回一个 ack，然后把自己的 rwnd 大小写入到 TCP 头部的 win 这个字段，这样发送方
就能根据这个字段来知道接收方的 rwnd 了。接下来，发送方在发送下一个 TCP segment 的时候，会先对比发送方的 cwnd 和接
收方的 rwnd，得出这二者之间的较小值，然后控制发送的 TCP segment 个数不能超过这个较小值。

那么，如何监控接收窗口太小导致无法收包的情况呢？
只需要检查SNMP计数：TCPZeroWindowDrop。如果系统中发生了接收窗口太小而无法收包的情况，就会产生该事件，然后该事件可以
通过 /proc/net/netstat 里的 TCPZeroWindowDrop 这个字段来查看。


因为 TCP 头部大小是有限制的，而其中的 win 这个字段只有 16bit，win 能够表示的大小最大只有 65535（64K），所以如果
想要支持更大的接收窗口以满足高性能网络，我们就需要打开下面这个配置项，系统中也是默认打开了该选项的：

```shell
net.ipv4.tcp_window_scaling = 1
```