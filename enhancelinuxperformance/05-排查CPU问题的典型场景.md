
---
排查CPU问题的典型场景
---

# 1 系统CPU使用率很高，但找不到高CPU使用率的进程，怎么办？
进程的pid在不断变化，说明什么？
第一个原因，进程在不停的崩溃重启，比如因为段错误，配置错误等等，这时，进程在退出后可能又被监控系统重启了。
第二个原因，这些进程都是短时进程，也就是在其他应用内部通过exec调用的外部命令。这些命令一般都只运行很短的时间就会结束，
我们很难用top这种间隔时间比较长的工具发现。
execsnoop是一个专为短时进程设计的工具。它通过ftrace实时监控进程的exec()行为，并输出短时进程的基本信息，包括进程
PID，父进程PID，命令行参数以及执行的结果。

碰到常规问题无法解释的CPU使用率情况时，首先要想到有可能是短时应用导致的问题，比如有可能是下面这两种情况:
第一，应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过top等工具也不容易发现。
第二，应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的CPU。
对于这类进程，我们可以使用pstree或者execsnoop找到它们的父进程，再从父进程所在的应用入手，排查问题的根源。'


# 2 系统中出现大量不可中断进程和僵尸进程该怎么办？

**进程状态**

R是Running或Runnable的缩写，表示进程在CPU的就绪队列中，正在运行或者正在等待运行。

D是Disk Sleep的缩写，也就是不可中断状态睡眠(Uninterruptible Sleep)，一般表示进程正在与硬件交互，并且交互过程
不允许被其他进程中断或打断。

Z是Zombie的缩写，表示僵尸进程，意思是进程实际上已经结束了，但是父进程还没有回收它的资源(比如进程的描述符，PID等)。

S是Interruptible Sleep的缩写，也就是可中断状态的睡眠。表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入R状态。

I是Idle的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。我们知道，硬件交互导致的不可中断进程用D表示，但对某些内核线程来说，
它们有可能实际上并没有任何负载，用Idle正是为了区分这种情况。要注意，D状态的进程会导致CPU的平均负载升高，但I状态的进程不会。

除了以上5个状态，进程还包括下面两种状态：
第一个是T或者t，也就是Stopped或Traced的缩写，表示进程处于暂停或者跟踪状态。
向一个进程发送SIGSTOP(19)信号，它就会因响应这个信号变成暂停状态(Stopped)，再向它发送SIGCONT信号()，进程又会恢复运行。

而当你用调试器(如gdb)调试一个进程时，在使用断点中断进程后，进程就会变成跟踪状态，这其实也是一种特殊的暂停状态，只不过你可以
用调试器来跟踪并按需要控制进程的运行。

第二个是X，也就是Dead的缩写，表示进程已经消亡，所以你不会在top或者ps命令中看到它。


对于不可中断状态，其实是为了保证进程数据与硬件状态一致，并且正常情况下，不可中断状态在很短时间内就会结束。所以，短时的
不可中断进程，我们一般可以忽略。

但如果系统或硬件发生了故障，进程可能会在不可中断状态保持良久，甚至导致系统中出现大量不可中断进程。这时，你就得注意下，
系统是不是出现了I/O等性能问题。

对于僵尸进程，则是多进程应用很容易碰到的问题。正常情况下，当一个进程创建了子进程后，它应该通过系统调用wait()或waitpid()
等待子进程结束，回收子进程的资源；而子进程在结束时，会向它的父进程发送SIGCHLD信号，所以，父进程还可以注册SIGCHLD信号的处理函数，
异步回收资源。

如果父进程没这么做，或是子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程。
通常，僵尸进程持续时间都会比较短，在父进程回收它的资源后就会消亡；或者父进程退出后，由init进程回收后也会消亡。
一旦父进程没有处理子进程的终止，还一直保持运行状态，那么子进程就会一直处于僵尸状态。大量的僵尸进程会耗尽PID进程号，导致新进程
不能创建，所以这种情况一定要避免。


# 3 iowait分析
我们可以使用dstat查看CPU和I/O这两种资源的使用情况。





![dstat.png](images%2Fdstat.png)





重点观察iowait升高(wai)时，磁盘的读请求(read)或写请求(write)是否很大，如果是，那说明iowait的升高跟磁盘的读或写请求
有关，很有可能就是磁盘读或写导致的。

那到底是哪个进程在读写磁盘呢？一般而言，我们首先会去排查处于不可中断状态的进程，即D进程。
从top的输出中找到处于D状态的进程号(pid)，然后使用pidstat工具查看这些进程的磁盘读写情况。

-d展示I/O统计数据，-p指定进程号，间隔1秒输出3组数据





![pidstat-io.png](images%2Fpidstat-io.png)





KB_rd表示每秒读的KB数，KB_wr表示每秒写的KB数，iodelay表示I/O的延迟，单位是时钟周期，如果为0，那么表示此进程
此时没有任何的读写。

如果没有发现问题，那咱们干脆用pidstat工具来观察所有进程的I/O使用情况。





![pidstat-d.png](images%2Fpidstat-d.png)





观察一会儿可以发现，是app进程在进行大量的磁盘读，每秒读取的数据有44MB，所以导致iowait升高的就是它，那么，app进程到底在执行啥I/O操作呢？
我们知道，进程要想访问磁盘，就必须通过系统调用，所以接下来的重点就是找出app进程的系统调用了。
**strace**正是最常用的跟踪进程系统调用的工具，所以我们根据pidstat输出中app进程的pid，使用strace来看一下app进程的系统调用情况。





![strace-pid.png](images%2Fstrace-pid.png)





结果出现了错误，strace命令执行失败了，并且输出的错误是没有权限，按理说我们所有操作都是以root用户运行了，为什么还会没有权限呢？
一般来说，遇到此类问题，我们可以先检查一下进程的状态是否正常。





![ps-check.png](images%2Fps-check.png)





果然，进程4736已经变成了Z状态，也就是僵尸进程。僵尸进程都是已经退出的进程，所以没法儿继续分析追踪它的系统调用。

至此，我们发现，iowait的问题仍然存在，但是top，pidstat这类工具已经不能给出更多的信息了。这时，我们就应该借助那些基于事件
记录的动态追踪工具了。


我们可以使用perf record -g 持续一会儿(比如15秒)，然后Ctrl+C退出，然后使用perf report查看报告。





![perf-record.png](images%2Fperf-record.png)





![perf-report-detail.png](images%2Fperf-report-detail.png)





我们可以看到，app进程的确在通过系统调用sys_read()来读取数据，并且从new_sync_read和blkdev_direct_IO能看出，app进程
正在读磁盘进行直接读，也就是绕过了系统缓存，每个读请求都会从磁盘直接读，这就可以解释我们观察到的iowait升高了。

看来，罪魁祸首是app进程内部进行了磁盘的直接I/O啊，下面的问题就容易解决了，我们接下来应该从代码层面分析，究竟是哪里出现了
直接读请求。

这里存在几个问题：
为什么不一上来就使用perf工具来排查解决问题，还要使用那么多其他的工具？
Perf report中，明明swapper占比最高(高达89%)，为什么我们会跳过它去分析app进程？





![swapper.png](images%2Fswapper.png)





原因是swapper是系统只在初始化时创建的init进程，之后，它就变成了一个最低优先级的空闲任务，也就是说，当CPU上没有其他任务
运行时，就会执行swapper。所以，我们可以称其为空闲任务。

在perf report的界面中，enter点击+展开swapper的调用栈，会看到，swapper的时钟事件都耗费在了do_idle上，也就是执行空闲任务。

所以我们会直接跳过它，直接去分析占用时钟事件第二的app进程。


在多任务系统中，占用时钟事件次数多的，未必就是性能瓶颈。所以，只观察到一个大数值，并不能说明什么问题。具体有没有瓶颈，还需要观测
多方面的多个指标，来交叉验证。另外，perf这种动态追踪工具，由于需要在系统内核中跟踪内核栈的各种事件，那么不可避免就会带来一定的
性能损失，而wmstat，pidstat这些是直接读取proc文件系统来获取性能指标的工具，不会带来性能损失。






![zombie-process.png](images%2Fzombie-process.png)





接下来，我们就来处理僵尸进程的问题，既然僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，要解决掉它们，就要找到它们的根儿，
也就是找出这些僵尸进程的父进程，然后在父进程里解决。

找父进程的方法很简单，使用pstree命令即可。
-a 表示输出命令行选项，-p表示PID，-s表示指定进程的父进程。





![pstree.png](images%2Fpstree.png)





运行完，你会发现6995号进程的父进程是6910，也就是app应用。
所以，我们需要查看app应用程序代码，看看子进程结束的处理是否正确，比如有没有调用wait()或waitpid()等待子进程结束，回收
子进程资源；抑或是，有没有注册SIGCHLD信号的处理函数。

如果perf的报告中，很多符号都不显示调用栈，怎么办？





![perf-call-graph.png](images%2Fperf-call-graph.png)





通过这个说明可以看到，-g选项等同于 --call-graph，它的参数是后面那些被逗号隔开的选项，意思分别是输出类型、最小阈值、输出限制、排序方法、排序关键词、分支以及值类型等。

堆栈显示不全，相关的参数当然就是最小阈值threshold，通过手册中对threshold的说明，我们知道，当一个事件发生比例高于这个阈值时，它的调用栈才会显示出来。
而threshold的默认值是0.5%，也就是说，只有CPU时间比例超过0.5%时，调用栈才能显示出来。在这种情况下，我们只需要给perf report设置一个小于0.34% 的阈值，就可以显示
我们想看到的调用图了。

```shell
perf report -g graph,0.3
```


# 4 总结
虽然这个案例是磁盘I/O导致了iowait升高，不过，iowait高不一定就代表I/O有性能瓶颈。当系统中只有I/O类型的进程在运行时，iowait也会很高，但实际上，磁盘的读写
可能远没有达到性能瓶颈的程度。

因此，遇到iowait升高时，需要先使用dstat，pidstat等工具，确认是否是磁盘I/O的问题，然后再找出到底是哪些进程导致了I/O升高。
等待I/O的进程一般是不可中断状态，所以用ps命令找到的D状态进程，即为可疑进程。但在这个案例中，在I/O操作后，进程又变成了僵尸进程，所以不能使用strace直接分析这个进程的系统调用。
在这种情况下，我们使用了perf工具，来分析系统的CPU时钟事件，最终发现是直接I/O导致的问题。此时，再检查应用源码中对应位置的问题，就很轻松了。

而僵尸进程的问题相对容易排查，使用pstree找出父进程后，去查看父进程的代码，检查wait()/waitpid()的调用，或是SIGCHLD信号处理函数的注册就行了。