
---
⾼性能IO模型：为什么单线程Redis能那么快？
---

今天，我们来探讨⼀个很多⼈都很关⼼的问题：“为什么单线程的Redis能那么快？”
⾸先，要厘清⼀个事实，我们通常说，Redis是单线程，主要是指Redis的⽹络IO和键值对读写是由⼀个线程来完成的，这也是Redis对外提供
键值存储服务的主要流程。但Redis的其他功能，⽐如持久化、 异步删除、集群数据同步等，其实是由额外的线程执⾏的。

所以，严格来说，Redis并不是单线程，但是我们⼀般把Redis称为单线程⾼性能，⽽且，这也会促使你紧接着提问：“为什么⽤单线程？
为什么单线程能这么快？”

要弄明⽩这个问题，我们就要深⼊地学习下Redis的单线程设计机制以及多路复⽤机制。之后你在调优Redis性能时，也能更有针对性地避免会
导致Redis单线程阻塞的操作，例如执⾏复杂度⾼的命令。

# 1 Redis为什么⽤单线程？

要更好地理解Redis为什么⽤单线程，我们就要先了解多线程的开销。

## 1.1 多线程的开销

⽇常写程序时，我们经常会听到⼀种说法：“使⽤多线程，可以增加系统吞吐率，或是可以增加系统扩展性。”的确，对于⼀个多线程的系统来说，
在有合理的资源分配的情况下，可以增加系统中处理请求操作的资源实体，进⽽提升系统能够同时处理的请求数，即吞吐率。

但是，请你注意，通常情况下，在我们采⽤多线程后，如果没有良好的系统设计，实际得到的结果，其实是不符合预期的。 我们刚开始增加线程数时，
系统吞吐率会增加，但是，再进⼀步增加线程时，系统吞吐率就增⻓迟缓了，有时甚⾄还会出现下降的情况。

为什么会出现这种情况呢？⼀个关键的瓶颈在于，系统中通常会存在被多线程同时访问的共享资源，⽐如⼀个共享的数据结构。当有多个线程要修改
这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进⾏保证，⽽这个额外的机制， 就会带来额外的开销。

并发访问控制⼀直是多线程开发中的⼀个难点问题，如果没有精细的设计，⽐如说，只是简单地采⽤⼀个粗粒度互斥锁，就会出现不理想的结果：
即使增加了线程，⼤部分线程也在等待获取访问共享资源的互斥锁， 并⾏变串⾏，系统吞吐率并没有随着线程的增加⽽增加。

⽽且，采⽤多线程开发⼀般会引⼊同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis
直接采⽤了单线程模式。

# 2 单线程Redis为什么那么快？

通常来说，单线程的处理能⼒要⽐多线程差很多，但是Redis却能使⽤单线程模型达到每秒数⼗万级别的处理能⼒，这是为什么呢？其实，这是
Redis多⽅⾯设计选择的⼀个综合结果。

⼀⽅⾯，Redis的⼤部分操作在内存上完成，再加上它采⽤了⾼效的数据结构，例如哈希表和跳表，这是它实现⾼性能的⼀个重要原因。另⼀⽅⾯，
就是Redis采⽤了多路复⽤机制，使其在⽹络IO操作中能并发处理⼤量的客⼾端请求，实现⾼吞吐率。接下来，我们就重点学习下多路复⽤机制。

⾸先，我们要弄明⽩⽹络操作的基本IO模型和潜在的阻塞点。毕竟，Redis采⽤单线程进⾏IO，如果线程被阻塞了，就⽆法进⾏多路复⽤了。


## 2.1 基本IO模型与阻塞点

以Get请求为例，Redis为了处理⼀个Get请求，需要监听客⼾端请求（bind/listen），和客⼾端建⽴连接（accept），从socket中读取请求
（recv），解析客⼾端发送请求（parse），根据请求类型读取键值数据（get），最后给客⼾端返回结果，即向socket中写回数据（send）。

下图显⽰了这⼀过程，其中，bind/listen、accept、recv、parse和send属于⽹络IO处理，⽽get属于键值数据操作。既然Redis是单线程，
那么，最基本的⼀种实现是在⼀个线程中依次执⾏上⾯说的这些操作。





![get-process.png](images%2Fget-process.png)





但是，在这⾥的⽹络IO操作中，有潜在的阻塞点，分别是accept()和recv()。当Redis监听到⼀个客⼾端有连接请求，但⼀直未能成功建⽴起连接时，
会阻塞在accept()函数这⾥，导致其他客⼾端⽆法和Redis建⽴连接。类似的，当Redis通过recv()从⼀个客⼾端读取数据时，如果数据⼀直没有到达，
Redis也会⼀直阻塞在recv()。 这就导致Redis整个线程阻塞，⽆法处理其他客⼾端请求，效率很低。不过，幸运的是，socket⽹络模型本⾝⽀持
⾮阻塞模式。


## 2.2 非阻塞模式

Socket⽹络模型的⾮阻塞模式设置，主要体现在三个关键的函数调⽤上，如果想要使⽤socket⾮阻塞模式，就必须要了解这三个函数的调⽤返回类型
和设置模式。接下来，我们就重点学习下它们。

在socket模型中，不同操作调⽤后会返回不同的套接字类型。socket()⽅法会返回主动套接字，然后调⽤listen()⽅法，将主动套接字转化为监听
套接字，此时，可以监听来⾃客⼾端的连接请求。最后，调⽤accept()⽅法接收到达的客⼾端连接，并返回已连接套接字。





![non-block-pattern.png](images%2Fnon-block-pattern.png)





针对监听套接字，我们可以设置⾮阻塞模式：当Redis调⽤accept()但⼀直未有连接请求到达时，Redis线程可以返回处理其他操作，⽽不⽤⼀直
等待。但是，你要注意的是，调⽤accept()时，已经存在监听套接字了。

虽然Redis线程可以不⽤继续等待，但是总得有机制继续在监听套接字上等待后续连接请求，并在有请求时通知Redis。

类似的，我们也可以针对已连接套接字设置⾮阻塞模式：Redis调⽤recv()后，如果已连接套接字上⼀直没有数据到达，Redis线程同样可以返回
处理其他操作。我们也需要有机制继续监听该已连接套接字，并在有数据达到时通知Redis。

这样才能保证Redis线程，既不会像基本IO模型中⼀直在阻塞点等待，也不会导致Redis⽆法处理实际到达的连接请求或数据。

到此，Linux中的IO多路复⽤机制就要登场了。


## 2.3 基于多路复⽤的⾼性能I/O模型

Linux中的IO多路复⽤机制是指⼀个线程处理多个IO流，就是我们经常听到的select/epoll机制。简单来说，在Redis只运⾏单线程的情况下，
该机制允许内核中，同时存在多个监听套接字和已连接套接字。内核会⼀直监听这些套接字上的连接请求或数据请求。⼀旦有请求到达，就会交给
Redis线程处理，这就实现了⼀个Redis线程处理多个IO流的效果。

下图就是基于多路复⽤的Redis IO模型。图中的多个FD就是刚才所说的多个套接字。Redis⽹络框架调⽤epoll机制，让内核监听这些套接字。
此时，Redis线程不会阻塞在某⼀个特定的监听或已连接套接字上，也就是说，不会阻塞在某⼀个特定的客⼾端请求处理上。正因为此，Redis
可以同时和多个客⼾端连接并处理请求，从⽽提升并发性。





![redis-io-pattern.png](images%2Fredis-io-pattern.png)





为了在请求到达时能通知到Redis线程，select/epoll提供了基于事件的回调机制，即针对不同事件的发⽣， 调⽤相应的处理函数。

那么，回调机制是怎么⼯作的呢？其实，select/epoll⼀旦监测到FD上有请求到达时，就会触发相应的事件。

这些事件会被放进⼀个事件队列，Redis单线程对该事件队列不断进⾏处理。这样⼀来，Redis⽆需⼀直轮询是否有请求实际发⽣，这就可以
避免造成CPU资源浪费。同时，Redis在对事件队列中的事件进⾏处理时，会调⽤相应的处理函数，这就实现了基于事件的回调。因为Redis
⼀直在对事件队列进⾏处理，所以能及时响应客⼾端请求，提升Redis的响应性能。

为了⽅便理解，我们再以连接请求和读数据请求为例，具体解释⼀下。

这两个请求分别对应Accept事件和Read事件，Redis分别对这两个事件注册accept和get回调函数。当Linux内核监听到有连接请求或读数据
请求时，就会触发Accept事件和Read事件，此时，内核就会回调Redis相应的accept和get函数进⾏处理。

这就像病⼈去医院瞧病。在医⽣实际诊断前，每个病⼈（等同于请求）都需要先分诊、测体温、登记等。如果这些⼯作都由医⽣来完成，医⽣的
⼯作效率就会很低。所以，医院都设置了分诊台，分诊台会⼀直处理这些诊断前的⼯作（类似于Linux内核监听请求），然后再转交给医⽣做
实际诊断。这样即使⼀个医⽣（相当于Redis单线程），效率也能提升。

不过，需要注意的是，即使你的应⽤场景中部署了不同的操作系统，多路复⽤机制也是适⽤的。因为这个机制的实现有很多种，既有基于Linux
系统下的select和epoll实现，也有基于FreeBSD的kqueue实现，以及基于Solaris的evport实现，这样，你可以根据Redis实际运⾏的
操作系统，选择相应的多路复⽤实现。


# 3 select、poll 与 epoll

前面提到 Redis 使用 IO 多路复用机制来实现高性能网络通信，而在 Linux 系统上，IO 多路复用有三种主要实现：select、poll 和 epoll。
理解它们的区别，是理解 Redis 为什么高性能的关键。


## 3.1 select

select 是最早的 IO 多路复用实现，其核心思路是：调用方将需要监听的文件描述符（fd）集合传递给内核，内核遍历所有 fd 检查是否有事件就绪，
然后返回就绪的 fd 数量，调用方再遍历一次 fd 集合找出具体是哪些 fd 就绪。

```C
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

**select 的局限性**：

1. **fd 数量受限**：`fd_set` 底层是一个位图（bitmap），大小由 `FD_SETSIZE` 宏定义，默认为 **1024**。这意味着 select 最多只能监听 1024 个 fd，
   对于高并发场景远远不够
2. **每次调用需全量拷贝**：每次调用 select 时，都需要将 fd_set 从用户态拷贝到内核态，调用结束后再从内核态拷贝回用户态。随着 fd 数量增多，
   拷贝开销线性增长
3. **内核遍历全量 fd**：内核并不知道哪些 fd 有事件就绪，只能线性遍历整个 fd_set，时间复杂度为 **O(N)**
4. **返回后需再次遍历**：select 返回的是就绪 fd 的数量，调用方还需要再遍历一次 fd_set 才能找到具体是哪些 fd 就绪


## 3.2 poll

poll 本质上和 select 做的事情一样，只是接口设计不同。poll 使用 `pollfd` 结构体数组代替了 `fd_set` 位图：

```C
struct pollfd {
    int fd;         // 文件描述符
    short events;   // 监听的事件类型
    short revents;  // 实际发生的事件类型
};

int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

**poll 相比 select 的改进**：
- **去掉了 1024 的 fd 数量限制**：pollfd 是一个数组，可以动态分配任意大小
- **输入输出分离**：events 和 revents 分开，不需要每次调用前重新设置 fd 集合

**poll 仍然存在的问题**：
- 每次调用仍需将全量 pollfd 数组在用户态和内核态之间拷贝
- 内核仍需线性遍历所有 fd 来检查事件，时间复杂度依然是 **O(N)**

所以，poll 解决了 select 的 fd 数量限制问题，但核心性能瓶颈并没有解决。


## 3.3 epoll

epoll 是 Linux 2.6 内核引入的，它从根本上解决了 select/poll 的性能问题。epoll 提供了三个系统调用：

```C
// 创建一个 epoll 实例，返回 epoll 文件描述符
int epoll_create(int size);

// 向 epoll 实例中添加、修改或删除要监听的 fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

// 等待事件就绪，返回就绪的事件列表
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

**epoll 的核心设计**：

**1. 红黑树管理 fd**

epoll 在内核中使用一棵**红黑树**来保存所有需要监听的 fd。当调用 `epoll_ctl` 添加一个 fd 时，内核将其插入红黑树，时间复杂度为 O(logN)。
与 select/poll 每次调用都要传递全量 fd 不同，epoll 只需要在 fd 变化时（新增连接或断开连接）调用一次 `epoll_ctl`，大幅减少了用户态与
内核态之间的数据拷贝。

**2. 就绪链表实现 O(1) 事件通知**

当某个 fd 上有事件就绪时，内核通过回调函数将该 fd 对应的 `epoll_event` 加入一个**就绪链表**（ready list）。调用 `epoll_wait` 时，
内核只需要检查这个就绪链表是否为空：如果不为空，就将就绪事件拷贝到用户态并返回；如果为空，就阻塞等待。

这意味着 `epoll_wait` 的时间复杂度是 **O(就绪 fd 数量)**，而不是 O(所有 fd 数量)。在高并发场景下，虽然可能有数万个连接，但同一时刻
就绪的连接通常只有少数几个，所以 epoll 的性能优势非常显著。

**3. mmap 减少拷贝开销**

epoll 通过 mmap 在用户态和内核态之间建立共享内存映射，避免了就绪事件从内核态到用户态的额外拷贝。


## 3.4 三者对比

| 特性 | select | poll | epoll |
|---|---|---|---|
| fd 数量限制 | 1024（FD_SETSIZE） | 无限制 | 无限制 |
| fd 传递方式 | 每次调用全量拷贝 fd_set | 每次调用全量拷贝 pollfd 数组 | epoll_ctl 增量注册，无需重复拷贝 |
| 内核检测方式 | 线性遍历所有 fd | 线性遍历所有 fd | 回调 + 就绪链表，仅处理就绪 fd |
| 时间复杂度 | O(N) | O(N) | O(就绪 fd 数量) |
| 适用场景 | fd 数量少且都很活跃 | fd 数量少且都很活跃 | 大量连接但活跃连接少（典型服务器场景） |

Redis 在 Linux 上默认使用 epoll，这也是 Redis 在 Linux 上性能最优的原因之一。


## 3.5 epoll 的 LT 与 ET 模式

epoll 支持两种事件触发模式：

**LT（Level Triggered，水平触发）**：只要 fd 上有未处理的就绪数据，每次调用 `epoll_wait` 都会返回该 fd。也就是说，如果你没有一次性
读完 fd 上的数据，下次 `epoll_wait` 还会通知你。这种模式编程简单，不容易丢数据。

**ET（Edge Triggered，边缘触发）**：只在 fd 的状态从"未就绪"变为"就绪"时通知一次。如果你没有一次性读完数据，`epoll_wait` 不会再次通知，
必须等到下一次新数据到达才会再次触发。因此，ET 模式下必须使用非阻塞 IO，并在收到通知后循环读取直到返回 EAGAIN。

**Redis 使用的是 LT 模式**。原因在于：
- LT 模式更安全，不会因为单次未读完数据而丢失事件
- Redis 的命令处理通常很快，每次事件处理都能及时读完数据，LT 模式的重复通知开销可以忽略
- LT 模式的代码逻辑更简单，降低了出错概率


# 4 Redis 事件驱动框架（ae 库）

Redis 并没有直接使用 epoll/select/kqueue 等系统调用，而是自己封装了一个轻量级的事件驱动库——**ae（A simple event-driven programming library）**。
ae 库屏蔽了不同操作系统的多路复用 API 差异，在 Linux 上使用 epoll，在 macOS/FreeBSD 上使用 kqueue，在 Solaris 上使用 evport，
最后兜底使用 select。


## 4.1 事件循环结构

ae 库的核心是 `aeEventLoop` 结构，它管理着 Redis 的整个事件循环：

```C
typedef struct aeEventLoop {
    int maxfd;                  // 当前注册的最大 fd
    int setsize;                // 可监听的最大 fd 数量
    long long timeEventNextId;  // 下一个时间事件的 ID
    aeFileEvent *events;        // 已注册的文件事件数组（按 fd 索引）
    aeFiredEvent *fired;        // 已就绪的事件数组
    aeTimeEvent *timeEventHead; // 时间事件链表头
    int stop;                   // 事件循环是否停止
    void *apidata;              // 多路复用 API 的私有数据（如 epoll fd）
    aeBeforeSleepProc *beforesleep;  // 每次阻塞等待前的回调
    aeBeforeSleepProc *aftersleep;   // 每次阻塞等待后的回调
} aeEventLoop;
```

ae 库处理两类事件：**文件事件**（File Event）和**时间事件**（Time Event）。


## 4.2 文件事件

文件事件是对套接字操作的抽象。每当一个套接字准备好执行 accept、read、write、close 等操作时，就会产生一个文件事件。

```C
typedef struct aeFileEvent {
    int mask;               // 事件类型掩码：AE_READABLE | AE_WRITABLE
    aeFileProc *rfileProc;  // 读事件处理函数
    aeFileProc *wfileProc;  // 写事件处理函数
    void *clientData;       // 客户端私有数据
} aeFileEvent;
```

Redis 服务器中常见的文件事件处理函数：

- **acceptTcpHandler**：监听套接字的读事件处理函数，用于接受新的客户端连接
- **readQueryFromClient**：已连接套接字的读事件处理函数，用于读取客户端发送的命令请求
- **sendReplyToClient**：已连接套接字的写事件处理函数，用于将命令回复写回客户端

当一个客户端连接到 Redis 时，会发生以下事件注册链路：
1. 监听套接字上产生 AE_READABLE 事件 → 调用 acceptTcpHandler → 创建客户端对象
2. 为该客户端的已连接套接字注册 AE_READABLE 事件 → 绑定 readQueryFromClient
3. 命令执行完毕后，为该套接字注册 AE_WRITABLE 事件 → 绑定 sendReplyToClient
4. 回复写完后，删除 AE_WRITABLE 事件注册


## 4.3 时间事件

时间事件用于处理需要在指定时间点执行的操作。Redis 中最重要的时间事件是 **serverCron**，它默认每 100 毫秒执行一次，负责：

- 更新服务器统计信息（内存使用、命中率等）
- 清理过期键
- 执行 AOF 重写、RDB 持久化等后台任务的状态检查
- 集群模式下的定期同步
- 关闭和清理失效的客户端连接

```C
typedef struct aeTimeEvent {
    long long id;                   // 时间事件 ID
    monotime when;                  // 事件触发的时间戳
    aeTimeProc *timeProc;           // 时间事件处理函数
    aeEventFinalizerProc *finalizerProc; // 事件删除时的清理函数
    void *clientData;               // 私有数据
    struct aeTimeEvent *prev;       // 前一个时间事件
    struct aeTimeEvent *next;       // 下一个时间事件
} aeTimeEvent;
```


## 4.4 事件调度流程

Redis 的事件循环主逻辑位于 `aeMain` 函数中，其伪代码如下：

```C
void aeMain(aeEventLoop *eventLoop) {
    eventLoop->stop = 0;
    while (!eventLoop->stop) {
        // 1. 执行 beforesleep 回调
        //    - 处理需要异步关闭的客户端
        //    - 将 AOF 缓冲区写入磁盘
        //    - 在 Redis 6.0+ 中，处理多线程 IO 读取的结果
        if (eventLoop->beforesleep != NULL)
            eventLoop->beforesleep(eventLoop);

        // 2. 调用 aeProcessEvents 处理文件事件和时间事件
        aeProcessEvents(eventLoop, AE_ALL_EVENTS | AE_CALL_BEFORE_SLEEP | AE_CALL_AFTER_SLEEP);
    }
}
```

`aeProcessEvents` 的处理逻辑：

1. 查找最近的时间事件，计算距离其触发还有多少时间，将这个时间作为 `epoll_wait` 的超时时间。这样既不会错过时间事件，
   也能在等待期间处理文件事件
2. 调用 `epoll_wait`（或对应平台的多路复用 API）阻塞等待文件事件就绪
3. 遍历所有就绪的文件事件，依次调用对应的读/写处理函数
4. 检查并执行所有已到期的时间事件

**文件事件与时间事件的优先级**：在同一次 `aeProcessEvents` 调用中，文件事件优先于时间事件被处理。这是因为文件事件涉及客户端请求，
需要尽快响应，而时间事件（如 serverCron）有一定的容错空间，晚几毫秒执行不会有问题。不过，由于 Redis 的命令执行速度很快，
时间事件通常不会有明显的延迟。


# 5 Redis 6.0 多线程 IO

Redis 从 6.0 版本开始引入了**多线程 IO** 机制。这是 Redis IO 模型的一次重大演进，但需要注意的是，**命令的执行仍然是单线程的**，
多线程仅用于网络数据的读取和写回。


## 5.1 为什么引入多线程

随着网络硬件性能的提升，Redis 的性能瓶颈逐渐从内存和 CPU 转移到了**网络 IO** 上。在高并发场景下，单线程处理网络读写成为了吞吐量的
主要限制因素。具体表现在：

- 从套接字中读取客户端请求数据（read 系统调用）
- 将回复数据写回套接字（write 系统调用）

这两个操作涉及用户态与内核态之间的数据拷贝，在大量并发连接时会消耗大量 CPU 时间。而命令的实际执行（在内存数据结构上的操作）反而
非常快。因此，Redis 6.0 将 IO 操作拆分到多个线程，让主线程更专注于命令执行。


## 5.2 多线程 IO 的工作流程

Redis 6.0 的多线程 IO 工作流程如下：

**阶段一：读取请求（多线程）**
1. 主线程通过 epoll_wait 获取所有就绪的读事件
2. 主线程将这些客户端连接**轮询分配**到各个 IO 线程的任务队列中
3. 主线程和 IO 线程并行读取各自负责的客户端请求数据
4. 主线程忙等（busy waiting）直到所有 IO 线程完成读取

**阶段二：执行命令（单线程）**
5. 主线程依次执行所有客户端的命令，这一步仍然是单线程串行的

**阶段三：写回响应（多线程）**
6. 主线程将需要写回的客户端连接分配到各个 IO 线程
7. 主线程和 IO 线程并行将命令回复写回各自负责的客户端
8. 主线程忙等直到所有 IO 线程完成写回

关键设计要点：
- **命令执行仍是单线程**：不需要加锁，不会产生并发问题，保持了 Redis 的简单性和线程安全性
- **IO 线程不执行命令**：IO 线程只负责网络数据的 read 和 write，不接触任何数据结构
- **同步屏障**：每个阶段之间通过忙等待实现同步，确保上一阶段完成后才进入下一阶段

**配置方式**：

```
# redis.conf

# 开启多线程 IO（默认关闭）
io-threads-do-reads yes

# IO 线程数量（包含主线程，建议设置为 CPU 核数的一半，最大值 128）
io-threads 4
```

官方建议：
- 4 核 CPU：io-threads 2~3
- 8 核 CPU：io-threads 4~6
- 设置过大反而会因为线程切换开销导致性能下降


## 5.3 Reactor 模式的演进

从设计模式的角度来看，Redis 的 IO 模型经历了一次 Reactor 模式的演进：

**Redis 6.0 之前——单 Reactor 单线程模型**

```
                    ┌─────────────────────┐
  客户端请求 ──────→│    Reactor（主线程）   │
                    │  epoll_wait 监听事件  │
                    │  读取请求数据         │
                    │  执行命令            │
                    │  写回响应数据         │
                    └─────────────────────┘
```

所有工作都在一个线程中完成。优点是实现简单、无锁，缺点是 IO 密集时吞吐量受限。

**Redis 6.0——单 Reactor 多线程模型**

```
                    ┌─────────────────────┐
  客户端请求 ──────→│    Reactor（主线程）   │
                    │  epoll_wait 监听事件  │
                    │  分发 IO 任务        │──────→ IO 线程 1（读/写）
                    │  执行命令（单线程）   │──────→ IO 线程 2（读/写）
                    │  分发 IO 任务        │──────→ IO 线程 3（读/写）
                    └─────────────────────┘
```

主线程（Reactor）仍然负责事件监听和命令执行，但将网络读写操作分发给多个 IO 线程并行处理。这样在不破坏 Redis 单线程命令执行
模型的前提下，显著提升了网络 IO 的吞吐能力。

根据官方基准测试，开启多线程 IO 后，Redis 的吞吐量可以提升一倍左右。


# 6 总结

Redis 的高性能 IO 模型可以从四个层次来理解：

| 层次 | 关键机制 | 解决的问题 |
|---|---|---|
| **单线程决策** | 避免多线程竞争和上下文切换 | 消除锁开销，保持代码简单 |
| **非阻塞 IO** | socket 非阻塞模式 | 线程不会阻塞在单个连接上 |
| **IO 多路复用** | epoll（红黑树 + 就绪链表） | 高效监听大量连接，O(就绪数) 复杂度 |
| **事件驱动框架** | ae 库（文件事件 + 时间事件） | 统一调度网络 IO 和定时任务 |
| **多线程 IO（6.0+）** | IO 线程并行读写，主线程串行执行 | 突破单线程网络 IO 的吞吐瓶颈 |

Redis 的设计哲学始终是：**在保持简单性的前提下最大化性能**。从一开始的单线程模型到 6.0 的多线程 IO，每一次演进都遵循这个原则——
只在瓶颈所在的地方引入复杂性，而不是全面多线程化。


