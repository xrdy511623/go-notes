
---
哨兵机制：主库挂了，如何不间断服务？
---

# 1 主库故障的影响

我们知道，在主从库集群模式下，如果从库发⽣故障了，客⼾端可以继续向主库或其他
从库发送请求，进⾏相关的操作，但是如果主库发⽣故障了，那就直接会影响到从库的同步，因为从库没有
相应的主库可以进⾏数据复制操作了。
⽽且，如果客⼾端发送的都是读操作请求，那还可以由从库继续提供服务，这在纯读的业务场景下还能被接
受。但是，⼀旦有写操作请求了，按照主从库模式下的读写分离要求，需要由主库来完成写操作。此时，也
没有实例可以来服务客⼾端的写操作请求了，如下图所⽰：





![master-down-situation.png](images%2Fmaster-down-situation.png)





⽆论是写服务中断，还是从库⽆法进⾏数据同步，都是不能接受的。所以，如果主库挂了，我们就需要运⾏
⼀个新主库，⽐如说把⼀个从库切换为主库，把它当成主库。这就涉及到三个问题：
1. 主库真的挂了吗？
2. 该选择哪个从库作为主库？
3. 怎么把新主库的相关信息通知给从库和客⼾端呢？

这就要提到哨兵机制了。在Redis主从集群中，哨兵机制是实现主从库⾃动切换的关键机制，它有效地解决
了主从复制模式下故障转移的这三个问题。

接下来，我们就⼀起学习下哨兵机制。


# 2 哨兵机制的基本流程

哨兵其实就是⼀个运⾏在特殊模式下的Redis进程，主从库实例运⾏的同时，它也在运⾏。哨兵主要负责的
就是三个任务：监控、选主（选择主库）和通知。

我们先看监控。监控是指哨兵进程在运⾏时，周期性地给所有的主从库发送PING命令，检测它们是否仍然
在线运⾏。如果从库没有在规定时间内响应哨兵的PING命令，哨兵就会把它标记为“下线状态”；同样，
如果主库也没有在规定时间内响应哨兵的PING命令，哨兵就会判定主库下线，然后开始⾃动切换主库的流程。

这个流程⾸先是执⾏哨兵的第⼆个任务，选主。主库挂了以后，哨兵就需要从很多个从库⾥，按照⼀定的规
则选择⼀个从库实例，把它作为新的主库。这⼀步完成后，现在的集群⾥就有了新主库。

然后，哨兵会执⾏最后⼀个任务：通知。在执⾏通知任务时，哨兵会把新主库的连接信息发给其他从库，让
它们执⾏replicaof命令，和新主库建⽴连接，并进⾏数据复制。同时，哨兵会把新主库的连接信息通知给客
⼾端，让它们把请求操作发到新主库上。

下面这张图展⽰了这三个任务以及它们各⾃的⽬标。





![switch-master-slave.png](images%2Fswitch-master-slave.png)





在这三个任务中，通知任务相对来说⽐较简单，哨兵只需要把新主库信息发给从库和客⼾端，让它们和新主
库建⽴连接就⾏，并不涉及决策的逻辑。但是，在监控和选主这两个任务中，哨兵需要做出两个决策：

在监控任务中，哨兵需要判断主库是否处于下线状态；
在选主任务中，哨兵也要决定选择哪个从库实例作为主库。

接下来，我们就先说说如何判断主库的下线状态。

你⾸先要知道的是，哨兵对主库的下线判断有“主观下线”和“客观下线”两种。那么，为什么会存在两种
判断呢？它们的区别和联系是什么呢？


## 2.1 主观下线和客观下线

先解释下什么是“主观下线”。

哨兵进程会使⽤PING命令检测它⾃⼰和主、从库的⽹络连接情况，⽤来判断实例的状态。如果哨兵发现主
库或从库对PING命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”。

如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就⾏了，因为从库的下线影响⼀般不太⼤，
集群的对外服务不会间断。

但是，如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可
能存在这么⼀个情况：那就是哨兵误判了，其实主库并没有故障。可是，⼀旦启动了主从切换，后续的选主
和通知操作都会带来额外的计算和通信开销。

为了避免这些不必要的开销，我们要特别注意误判的情况。

⾸先，我们要知道啥叫误判。很简单，就是主库实际并没有下线，但是哨兵误以为它下线了。误判⼀般会发
⽣在集群⽹络压⼒较⼤、⽹络拥塞，或者是主库本⾝压⼒较⼤的情况下。

⼀旦哨兵判断主库下线了，就会开始选择新主库，并让从库和新主库进⾏数据同步，这个过程本⾝就会有开
销，例如，哨兵要花时间选出新主库，从库也需要花时间和新主库同步。⽽在误判的情况下，主库本⾝根本
就不需要进⾏切换的，所以这个过程的开销是没有价值的。正因为这样，我们需要判断是否有误判，以及减
少误判。

那怎么减少误判呢？在⽇常⽣活中，当我们要对⼀些重要的事情做判断的时候，经常会和家⼈或朋友⼀起商
量⼀下，然后再做决定。

哨兵机制也是类似的，它通常会采⽤多实例组成的集群模式进⾏部署，这也被称为哨兵集群。引⼊多个哨兵
实例⼀起来判断，就可以避免单个哨兵因为⾃⾝⽹络状况不好，⽽误判主库下线的情况。同时，多个哨兵的
⽹络同时不稳定的概率较⼩，由它们⼀起做决策，误判率也能降低。

在判断主库是否下线时，不能由⼀个哨兵说了算，只有⼤多数的哨兵实例，都判断主库已经“主观下
线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为⼀个客观事实了。这个判断原则
就是：少数服从多数。同时，这会进⼀步触发哨兵开始主从切换流程。

下面这张图展⽰了这⾥的逻辑。

如下图所⽰，Redis主从集群有⼀个主库、三个从库，还有三个哨兵实例。在图⽚的左边，哨兵2判断主库
为“主观下线”，但哨兵1和3却判定主库是上线状态，此时，主库仍然被判断为处于上线状态。在图⽚的右边，
哨兵1和2都判断主库为“主观下线”，此时，即使哨兵3仍然判断主库为上线状态，主库也被标记为“客观下线”了。





![subject-and-object-down.png](images%2Fsubject-and-object-down.png)





简单来说，”客观下线”的标准就是，当有 N 个哨兵实例时，需要有 **quorum** 个实例判断主库为”主观下线”，
才能最终判定主库为”客观下线”。quorum 的值在配置哨兵时通过 `sentinel monitor` 命令指定，通常设置为 N/2 + 1（如 3 个哨兵设 quorum 为 2，
5 个哨兵设 quorum 为 3）。这样既可以减少误判的概率，也能避免误判带来的⽆谓的主从库切换。

好了，到这⾥，你可以看到，借助于多个哨兵实例的共同判断机制，我们就可以更准确地判断出主库是否处
于下线状态。如果主库的确下线了，哨兵就要开始下⼀个决策过程了，即从许多从库中，选出⼀个从库来做
新主库。

## 2.2 如何选定新主库？

⼀般来说，我把哨兵选择新主库的过程称为“筛选+打分”。简单来说，我们在多个从库中，先按照⼀定的
筛选条件，把不符合条件的从库去掉。然后，我们再按照⼀定的规则，给剩下的从库逐个打分，将得分最⾼
的从库选为新主库，如下图所⽰：





![how-to-select-new-master.png](images%2Fhow-to-select-new-master.png)





在刚刚的这段话⾥，需要注意的是两个“⼀定”，现在，我们要考虑这⾥的“⼀定”具体是指什么。
⾸先来看筛选的条件。

⼀般情况下，我们肯定要先保证所选的从库仍然在线运⾏。不过，在选主时从库正常在线，这只能表⽰从库
的现状良好，并不代表它就是最适合做主库的。

设想⼀下，如果在选主时，⼀个从库正常运⾏，我们把它选为新主库开始使⽤了。可是，很快它的⽹络出了
故障，此时，我们就得重新选主了。这显然不是我们期望的结果。

所以，在选主时，除了要检查从库的当前在线状态，还要判断它之前的⽹络连接状态。如果从库总是和主库
断连，⽽且断连次数超出了⼀定的阈值，我们就有理由相信，这个从库的⽹络状况并不是太好，就可以把这
个从库筛掉了。

具体怎么判断呢？你使⽤配置项down-after-milliseconds * 10。其中，down-after-milliseconds是我们认
定主从库断连的最⼤连接超时时间。如果在down-after-milliseconds毫秒内，主从节点都没有通过⽹络联
系上，我们就可以认为主从节点断连了。如果发⽣断连的次数超过了10次，就说明这个从库的⽹络状况不
好，不适合作为新主库。

好了，这样我们就过滤掉了不适合做主库的从库，完成了筛选⼯作。

接下来就要给剩余的从库打分了。我们可以分别按照三个规则依次进⾏三轮打分，这三个规则分别是从库优
先级、从库复制进度以及从库ID号。只要在某⼀轮中，有从库得分最⾼，那么它就是主库了，选主过程到此
结束。如果没有出现得分最⾼的从库，那么就继续进⾏下⼀轮。

第⼀轮：优先级最⾼的从库得分⾼。

⽤⼾可以通过slave-priority配置项，给不同的从库设置不同优先级。⽐如，你有两个从库，它们的内存⼤
⼩不⼀样，你可以⼿动给内存⼤的实例设置⼀个⾼优先级。在选主时，哨兵会给优先级⾼的从库打⾼分，如
果有⼀个从库优先级最⾼，那么它就是新主库了。如果从库的优先级都⼀样，那么哨兵开始第⼆轮打分。

第⼆轮：和旧主库同步程度最接近的从库得分⾼。

这个规则的依据是，如果选择和旧主库同步最接近的那个从库作为主库，那么，这个新主库上就有最新的数
据。

如何判断从库和旧主库间的同步进度呢？

主从库同步时有个命令传播的过程。在这个过程中，主库会⽤master_repl_offset记
录当前的最新写操作在repl_backlog_buffer中的位置，⽽从库会⽤slave_repl_offset这个值记录当前的复
制进度。

此时，我们想要找的从库，它的slave_repl_offset需要最接近master_repl_offset。如果在所有从库中，有
从库的slave_repl_offset最接近master_repl_offset，那么它的得分就最⾼，可以作为新主库。

就像下图所⽰，旧主库的master_repl_offset是1000，从库1、2和3的slave_repl_offset分别是950、990和
900，那么，从库2就应该被选为新主库。






![slave-repl-offset-comparsion.png](images%2Fslave-repl-offset-comparsion.png)





当然，如果有两个从库的slave_repl_offset值⼤⼩是⼀样的（例如，从库1和从库2的slave_repl_offset值都
是990），我们就需要给它们进⾏第三轮打分了。

第三轮：ID号⼩的从库得分⾼。

每个实例都会有⼀个ID，这个ID就类似于这⾥的从库的编号。⽬前，Redis在选主库时，有⼀个默认的规
定：在优先级和复制进度都相同的情况下，ID号最⼩的从库得分最⾼，会被选为新主库。

到这⾥，新主库就被选出来了，“选主”这个过程就完成了。

我们再回顾下这个流程。⾸先，哨兵会按照在线状态、⽹络状态，筛选过滤掉⼀部分不符合要求的从库，然
后，依次按照优先级、复制进度、ID号⼤⼩再对剩余的从库进⾏打分，只要有得分最⾼的从库出现，就把它
选为新主库。

# 3 故障切换的执行过程

前面介绍了哨兵如何判断主库下线以及如何选出新主库，接下来我们看看选出新主库后，哨兵是如何完成整个切换过程的。

## 3.1 切换步骤

当哨兵集群选出了执行切换的 leader 哨兵（leader 选举机制在下一篇文章中详细讨论），leader 哨兵会按以下步骤执行故障切换：

**第一步：让新主库升级为主库**

leader 哨兵向选出的新主库发送 `SLAVEOF NO ONE` 命令，使其停止复制，成为独立的主库。然后 leader 哨兵会持续发送
`INFO` 命令检查新主库的角色，直到其 role 变为 master，确认升级成功。

**第二步：让其他从库指向新主库**

leader 哨兵向其他从库发送 `REPLICAOF <新主库IP> <新主库端口>` 命令，让它们开始从新主库同步数据。

这里有一个重要的配置参数 `sentinel parallel-syncs`，它控制同时执行同步的从库数量。例如设置为 1，
表示每次只让 1 个从库与新主库同步，其他从库排队等待。这样做是为了避免所有从库同时执行全量同步，
导致新主库的网络带宽和 fork 开销过大，影响新主库正常服务。

**第三步：通知客户端**

leader 哨兵通过 Pub/Sub 机制发布 `+switch-master` 事件，事件中包含新主库的 IP 和端口。
支持哨兵模式的客户端库（如 Jedis、go-redis、redis-py）会订阅这个事件，自动切换到新主库的连接。

**第四步：处理旧主库**

哨兵会将旧主库的信息记录下来，并持续监控它。当旧主库恢复上线后，leader 哨兵会向它发送
`REPLICAOF <新主库IP> <新主库端口>` 命令，**将旧主库自动降级为新主库的从库**。
旧主库会清空自身数据，从新主库执行全量同步，成为一个普通的从库节点。

这意味着故障切换是不可逆的——即使旧主库恢复，它也不会重新变回主库。


## 3.2 故障切换的时间线

一次完整的故障切换，从主库真正宕机到客户端感知到新主库，涉及以下时间开销：

```
主库宕机
  │
  ├─ 哨兵检测到主库无响应（down-after-milliseconds，默认 30 秒）
  │
  ├─ 多个哨兵确认”客观下线”（几秒内完成投票）
  │
  ├─ 哨兵集群选出 leader（几秒内完成 Raft 类选举）
  │
  ├─ leader 执行故障切换（选主 + SLAVEOF NO ONE + 通知从库，几秒）
  │
  └─ 客户端感知并切换连接（取决于客户端实现和订阅延迟）
```

总体来说，从主库宕机到服务恢复，通常需要 **30~60 秒**（主要受 `down-after-milliseconds` 配置影响）。
如果业务对可用性要求极高，可以适当降低 `down-after-milliseconds` 的值，但过低会增加误判风险。


# 4 哨兵配置详解

## 4.1 核心配置项

```
# 哨兵监控的主库，<master-name> 是自定义名称，<quorum> 是判定客观下线的票数
sentinel monitor <master-name> <ip> <port> <quorum>
# 示例：监控名为 mymaster 的主库，quorum 为 2
sentinel monitor mymaster 172.16.19.3 6379 2

# 判定主库主观下线的超时时间（毫秒），默认 30000（30秒）
sentinel down-after-milliseconds mymaster 30000

# 故障切换超时时间（毫秒），默认 180000（3分钟）
# 超过此时间未完成切换，会取消本次切换并在之后重试
sentinel failover-timeout mymaster 180000

# 故障切换后，同时执行全量同步的从库数量，默认 1
# 值越大恢复越快，但对新主库压力越大
sentinel parallel-syncs mymaster 1

# 主库连接密码（如果主库设置了 requirepass）
sentinel auth-pass mymaster your-password

# 当发生故障切换时执行的脚本（可选）
sentinel notification-script mymaster /path/to/notify.sh
sentinel client-reconfig-script mymaster /path/to/reconfig.sh
```

## 4.2 部署建议

**哨兵数量**：至少部署 **3 个**哨兵实例，且应部署在不同的物理机或可用区上，避免单点故障。
常见的部署方案：

| 哨兵数量 | quorum | 容忍故障数 | 说明 |
|---|---|---|---|
| 3 | 2 | 1 | 最常见的生产部署方案 |
| 5 | 3 | 2 | 更高可用性，适合关键业务 |

**网络隔离注意**：quorum 个哨兵判定”客观下线”后，还需要**多数哨兵（N/2+1）** 同意才能选出 leader 执行切换。
也就是说，quorum 只决定”是否判定客观下线”，而 leader 选举始终需要多数派同意。因此即使 quorum 设为 1，
也至少需要 2 个哨兵在线（3 哨兵集群中）才能完成切换。

**不要与数据节点混部**：哨兵应独立部署，不要和 Redis 数据节点部署在同一台机器上。
否则机器宕机时，会同时失去哨兵和数据节点，降低了故障检测能力。


# 5 客户端如何连接哨兵模式

在哨兵模式下，客户端不再直接连接 Redis 主库的固定地址，而是连接哨兵集群，由哨兵告知当前主库的地址。
以下是几种常见客户端库的连接方式：

**Go（go-redis）**：

```go
rdb := redis.NewFailoverClient(&redis.FailoverOptions{
    MasterName:    “mymaster”,
    SentinelAddrs: []string{
        “172.16.19.10:26379”,
        “172.16.19.11:26379”,
        “172.16.19.12:26379”,
    },
})
```

**Java（Jedis）**：

```java
Set<String> sentinels = new HashSet<>();
sentinels.add(“172.16.19.10:26379”);
sentinels.add(“172.16.19.11:26379”);
sentinels.add(“172.16.19.12:26379”);
JedisSentinelPool pool = new JedisSentinelPool(“mymaster”, sentinels);
```

**Python（redis-py）**：

```python
from redis.sentinel import Sentinel

sentinel = Sentinel([
    ('172.16.19.10', 26379),
    ('172.16.19.11', 26379),
    ('172.16.19.12', 26379),
])
master = sentinel.master_for('mymaster')
master.set('key', 'value')
```

客户端库内部会：
1. 连接哨兵，通过 `SENTINEL get-master-addr-by-name` 命令获取当前主库地址
2. 订阅哨兵的 `+switch-master` 频道，当主库切换时自动更新连接


# 6 总结

哨兵机制通过三大功能实现了主从库的自动切换：

| 功能 | 关键机制 | 核心配置 |
|---|---|---|
| **监控** | PING 命令探活 → 主观下线 → quorum 票数达标 → 客观下线 | `down-after-milliseconds`、`sentinel monitor` 的 quorum |
| **选主** | 筛选（在线 + 网络稳定）→ 打分（优先级 > 复制进度 > ID） | `replica-priority` |
| **切换** | SLAVEOF NO ONE → REPLICAOF 新主库 → Pub/Sub 通知客户端 | `parallel-syncs`、`failover-timeout` |
| **善后** | 旧主库恢复后自动降级为从库 | - |

**生产要点**：
- 至少部署 3 个哨兵，quorum 设为 2
- `down-after-milliseconds` 根据业务可用性要求调整（通常 5~30 秒）
- `parallel-syncs` 设为 1，避免切换时集中全量同步
- 客户端使用哨兵模式连接，而非直连固定 IP

哨兵机制解决了”自动发现故障并切换”的问题，但哨兵集群本身是如何协调工作的？比如由哪个哨兵来执行切换、
哨兵之间如何互相发现？这些问题将在下一篇”哨兵集群的关键机制”中详细讨论。