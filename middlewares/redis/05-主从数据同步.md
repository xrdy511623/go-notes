
---
数据同步：主从库如何实现数据一致？
---

# 1 主从库模式与读写分离

通过AOF和RDB持久化机制，如果Redis发⽣了宕机，它们可以分别通过回放⽇志和重新读⼊RDB⽂
件的⽅式恢复数据，从⽽保证尽量少丢失数据，提升可靠性。

不过，即使⽤了这两种⽅法，也依然存在服务不可⽤的问题。⽐如说，我们在实际使⽤时只运⾏了⼀个
Redis实例，那么，如果这个实例宕机了，它在恢复期间，是⽆法服务新来的数据存取请求的。

那我们总说的Redis具有⾼可靠性，⼜是什么意思呢？其实，这⾥有两层含义：⼀是数据尽量少丢失，⼆是
服务尽量少中断。AOF和RDB保证了前者，⽽对于后者，Redis的做法就是增加副本冗余量，将⼀份数据同
时保存在多个实例上。即使有⼀个实例出现了故障，需要过⼀段时间才能恢复，其他实例也可以对外提供服
务，不会影响业务使⽤。

多实例保存同⼀份数据，听起来好像很不错，但是，我们必须要考虑⼀个问题：这么多副本，它们之间的数
据如何保持⼀致呢？数据读写操作可以发给所有的实例吗？

实际上，Redis提供了主从库模式，以保证数据副本的⼀致，主从库之间采⽤的是读写分离的⽅式。
读操作：主库、从库都可以接收；
写操作：⾸先到主库执⾏，然后，主库将写操作同步给从库。





![master-slave-pattern.png](images%2Fmaster-slave-pattern.png)






那么，为什么要采⽤读写分离的⽅式呢？

你可以设想⼀下，如果在上图中，不管是主库还是从库，都能接收客⼾端的写操作，那么，⼀个直接的问题
就是：如果客⼾端对同⼀个数据（例如k1）前后修改了三次，每⼀次的修改请求都发送到不同的实例上，在
不同的实例上执⾏，那么，这个数据在这三个实例上的副本就不⼀致了（分别是v1、v2和v3）。在读取这
个数据的时候，就可能读取到旧的值。

如果我们⾮要保持这个数据在三个实例上⼀致，就要涉及到加锁、实例间协商是否完成修改等⼀系列操作，
但这会带来巨额的开销，当然是不太能接受的。

⽽主从库模式⼀旦采⽤了读写分离，所有数据的修改只会在主库上进⾏，不⽤协调三个实例。主库有了最新
的数据后，会同步给从库，这样，主从库的数据就是⼀致的。

那么，主从库同步是如何完成的呢？主库数据是⼀次性传给从库，还是分批同步？要是主从库间的⽹络断连
了，数据还能保持⼀致吗？下面讲讲主从库同步的原理，以及应对⽹络断连⻛险的⽅案。

好了，我们先来看看主从库间的第⼀次同步是如何进⾏的，这也是Redis实例建⽴主从库模式后的规定动
作。


# 2 第一次同步：全量复制

当我们启动多个Redis实例的时候，它们相互之间就可以通过replicaof（Redis 5.0之前使⽤slaveof）命令形
成主库和从库的关系，之后会按照三个阶段完成数据的第⼀次同步。
例如，现在有实例1（ip：172.16.19.3）和实例2（ip：172.16.19.5），我们在实例2上执⾏以下这个命令
后，实例2就变成了实例1的从库，并从实例1上复制数据：

```shell
replicaof 172.16.19.3 6379
```

接下来，我们就要学习主从库间数据第⼀次同步的三个阶段了。可以先看⼀下下⾯这张图，有个整体感
知，接下来我再具体介绍。





![first-sync.png](images%2Ffirst-sync.png)





第⼀阶段是主从库间建⽴连接、协商同步的过程，主要是为全量复制做准备。在这⼀步，从库和主库建⽴起
连接，并告诉主库即将进⾏同步，主库确认回复后，主从库间就可以开始同步了。

具体来说，从库给主库发送psync命令，表⽰要进⾏数据同步，主库根据这个命令的参数来启动复制。
psync命令包含了主库的runID和复制进度offset两个参数。

runID，是每个Redis实例启动时都会⾃动⽣成的⼀个随机ID，⽤来唯⼀标记这个实例。当从库和主库第⼀
次复制时，因为不知道主库的runID，所以将runID设为“？”。

offset，此时设为-1，表⽰第⼀次复制。

主库收到psync命令后，会⽤FULLRESYNC响应命令带上两个参数：主库runID和主库⽬前的复制进度
offset，返回给从库。从库收到响应后，会记录下这两个参数。

这⾥有个地⽅需要注意，FULLRESYNC响应表⽰第⼀次复制采⽤的全量复制，也就是说，主库会把当前所
有的数据都复制给从库。

在第⼆阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快
照⽣成的RDB⽂件。

具体来说，主库执⾏bgsave命令，⽣成RDB⽂件，接着将⽂件发给从库。从库接收到RDB⽂件后，会先清
空当前数据库，然后加载RDB⽂件。这是因为从库在通过replicaof命令开始和主库同步前，可能保存了其他
数据。为了避免之前数据的影响，从库需要先把当前数据库清空。

在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis的服务就被中
断了。但是，这些请求中的写操作并没有记录到刚刚⽣成的RDB⽂件中。为了保证主从库的数据⼀致性，主
库会在内存中⽤专⻔的replication buffer，记录RDB⽂件⽣成后收到的所有写操作。

最后，也就是第三个阶段，主库会把第⼆阶段执⾏过程中新收到的写命令，再发送给从库。具体的操作是，
当主库完成RDB⽂件发送后，就会把此时replication buffer中的修改操作发给从库，从库再重新执⾏这些操
作。这样⼀来，主从库就实现同步了。

# 3 主从级联模式分担全量复制时的主库压⼒

通过分析主从库间第⼀次数据同步的过程，你可以看到，⼀次全量复制中，对于主库来说，需要完成两个耗
时的操作：⽣成RDB⽂件和传输RDB⽂件。

如果从库数量很多，⽽且都要和主库进⾏全量复制的话，就会导致主库忙于fork⼦进程⽣成RDB⽂件，进⾏
数据全量同步。fork这个操作会阻塞主线程处理正常请求，从⽽导致主库响应应⽤程序的请求速度变慢。此
外，传输RDB⽂件也会占⽤主库的⽹络带宽，同样会给主库的资源使⽤带来压⼒。那么，有没有好的解决⽅
法可以分担主库压⼒呢？

其实是有的，这就是“主-从-从”模式。

在刚才介绍的主从库模式中，所有的从库都是和主库连接，所有的全量复制也都是和主库进⾏的。现在，我
们可以通过“主-从-从”模式将主库⽣成RDB和传输RDB的压⼒，以级联的⽅式分散到从库上。

简单来说，我们在部署主从集群的时候，可以⼿动选择⼀个从库（⽐如选择内存资源配置较⾼的从库），⽤
于级联其他的从库。然后，我们可以再选择⼀些从库（例如三分之⼀的从库），在这些从库上执⾏如下命
令，让它们和刚才所选的从库，建⽴起主从关系。

```shell
replicaof 所选从库的IP 6379
```

这样⼀来，这些从库就会知道，在进⾏同步时，不⽤再和主库进⾏交互了，只要和级联的从库进⾏写操作同
步就⾏了，这就可以减轻主库上的压⼒，如下图所⽰：





![class-link-pattern.png](images%2Fclass-link-pattern.png)





好了，到这⾥，我们了解了主从库间通过全量复制实现数据同步的过程，以及通过“主-从-从”模式分担主
库压⼒的⽅式。那么，⼀旦主从库完成了全量复制，它们之间就会⼀直维护⼀个⽹络连接，主库会通过这个
连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于⻓连接的命令传播，可以避免频繁建⽴
连接的开销。

听上去好像很简单，但不可忽视的是，这个过程中存在着⻛险点，最常⻅的就是⽹络断连或阻塞。如果⽹络
断连，主从库之间就⽆法进⾏命令传播了，从库的数据⾃然也就没办法和主库保持⼀致了，客⼾端就可能从
从库读到旧数据。

接下来，我们就来聊聊⽹络断连后的解决办法。

# 4 网络断连后的增量复制

在Redis 2.8之前，如果主从库在命令传播时出现了⽹络闪断，那么，从库就会和主库重新进⾏⼀次全量复
制，开销⾮常⼤。

从Redis 2.8开始，⽹络断了之后，主从库会采⽤增量复制的⽅式继续同步。听名字⼤概就可以猜到它和全
量复制的不同：全量复制是同步所有数据，⽽增量复制只会把主从库⽹络断连期间主库收到的命令，同步给
从库。

那么，增量复制时，主从库之间具体是怎么保持同步的呢？这⾥的奥妙就在于repl_backlog_buffer这个缓
冲区。我们先来看下它是如何⽤于增量命令的同步的。

当主从库断连后，主库会把断连期间收到的写操作命令，写⼊replication buffer，同时也会把这些操作命令
也写⼊repl_backlog_buffer这个缓冲区。

repl_backlog_buffer是⼀个环形缓冲区，主库会记录⾃⼰写到的位置，从库则会记录⾃⼰已经读到的位置。

刚开始的时候，主库和从库的写读位置在⼀起，这算是它们的起始位置。随着主库不断接收新的写操作，它
在缓冲区中的写位置会逐步偏离起始位置，我们通常⽤偏移量来衡量这个偏移距离的⼤⼩，对主库来说，对
应的偏移量就是master_repl_offset。主库接收的新写操作越多，这个值就会越⼤。

同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已
复制的偏移量slave_repl_offset也在不断增加。正常情况下，这两个偏移量基本相等。





![repl-backlog-buffer.png](images%2Frepl-backlog-buffer.png)





主从库的连接恢复之后，从库⾸先会给主库发送psync命令，并把⾃⼰当前的slave_repl_offset发给主库，
主库会判断⾃⼰的master_repl_offset和slave_repl_offset之间的差距。

在⽹络断连阶段，主库可能会收到新的写操作命令，所以，⼀般来说，master_repl_offset会⼤于
slave_repl_offset。此时，主库只⽤把master_repl_offset和slave_repl_offset之间的命令操作同步给从库
就⾏。

就像刚刚⽰意图的中间部分，主库和从库之间相差了put d e和put d f两个操作，在增量复制时，主库只需
要把它们同步给从库，就⾏了。

说到这⾥，我们再借助⼀张图，回顾下增量复制的流程。






![increment-replication.png](images%2Fincrement-replication.png)





不过，有⼀个地⽅我要强调⼀下，因为repl_backlog_buffer是⼀个环形缓冲区，所以在缓冲区写满后，主
库会继续写⼊，此时，就会覆盖掉之前写⼊的操作。如果从库的读取速度⽐较慢，就有可能导致从库还未读
取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不⼀致。

因此，我们要想办法避免这⼀情况，⼀般⽽⾔，我们可以调整repl_backlog_size这个参数。这个参数和所
需的缓冲空间⼤⼩有关。缓冲空间的计算公式是：缓冲空间⼤⼩ = 主库写⼊命令速度 * 操作⼤⼩ - 主从库间
⽹络传输命令速度 * 操作⼤⼩。在实际应⽤中，考虑到可能存在⼀些突发的请求压⼒，我们通常需要把这个
缓冲空间扩⼤⼀倍，即repl_backlog_size = 缓冲空间⼤⼩ * 2，这也就是repl_backlog_size的最终值。

举个例⼦，如果主库每秒写⼊2000个操作，每个操作的⼤⼩为2KB，⽹络每秒能传输1000个操作，那么，
有1000个操作需要缓冲起来，这就⾄少需要2MB的缓冲空间。否则，新写的命令就会覆盖掉旧操作了。为
了应对可能的突发压⼒，我们最终把repl_backlog_size设为4MB。

这样⼀来，增量复制时主从库的数据不⼀致⻛险就降低了。不过，如果并发请求量⾮常⼤，连两倍的缓冲空
间都存不下新操作请求的话，此时，主从库数据仍然可能不⼀致。

针对这种情况，⼀⽅⾯，你可以根据Redis所在服务器的内存资源再适当增加repl_backlog_size值，⽐如说
设置成缓冲空间⼤⼩的4倍，另⼀⽅⾯，你可以考虑使⽤切⽚集群来分担单个主库的请求压⼒。


# 5 replication buffer 与 repl_backlog_buffer 的区别

在前面的内容中，我们分别提到了 replication buffer 和 repl_backlog_buffer 这两个缓冲区，它们的名字很像，但职责完全不同，
是主从复制中最容易混淆的概念。

| 对比维度 | replication buffer | repl_backlog_buffer |
|---|---|---|
| **归属** | 每个从库各自拥有一份 | 主库全局唯一，所有从库共享 |
| **生命周期** | 从库连接时创建，断开时销毁 | 主库启动后一直存在（有从库连接过就不会释放） |
| **用途** | 全量复制期间，缓存 RDB 生成后主库新收到的写命令，待 RDB 发送完毕后将缓存命令发给从库 | 增量复制时，记录主库最近执行的写命令，供断连恢复的从库读取差异数据 |
| **数据结构** | 动态增长的链表 | 固定大小的环形缓冲区 |
| **溢出后果** | 从库的 replication buffer 超过限制（`client-output-buffer-limit replica`）时，主库会断开该从库连接，从库需要重新全量同步 | 环形缓冲区被覆盖时，落后太多的从库无法增量同步，需要重新全量同步 |
| **大小控制** | `client-output-buffer-limit replica 256mb 64mb 60` | `repl-backlog-size`（默认 1MB） |

简单来说：**replication buffer 服务于全量复制阶段，repl_backlog_buffer 服务于增量复制阶段**。

一个常见的生产问题是：全量同步时如果主库写入量很大，replication buffer 会持续增长。当它超过 `client-output-buffer-limit`
的硬限制时，主库会强制断开从库连接。而断开后从库又会重新发起全量同步，形成恶性循环。因此，对于写入量大的主库，
需要适当调大 `client-output-buffer-limit replica` 的值。


# 6 异步复制与数据一致性

## 6.1 异步复制的风险

Redis 的主从复制默认是**异步**的。也就是说，主库执行完写命令并返回客户端成功后，才会异步地将命令传播给从库。
主库不会等待从库确认收到并执行完命令。

这种异步设计带来了高性能，但也意味着存在**数据丢失的风险窗口**：

1. 客户端向主库写入数据，主库返回 OK
2. 主库将命令发送给从库之前，主库宕机
3. 从库被提升为新主库，但它没有收到最后那条写命令
4. 数据丢失

这是 Redis 主从复制中一个基本的权衡：**用数据安全性换取性能**。Redis 的作者认为，对于大多数场景来说，
异步复制带来的性能优势远大于极小概率的数据丢失风险。


## 6.2 WAIT 命令：同步等待

如果业务场景对数据安全性有更高要求，Redis 提供了 `WAIT` 命令来实现**同步等待**：

```
WAIT <numreplicas> <timeout>
```

- `numreplicas`：等待至少多少个从库确认收到了之前的写命令
- `timeout`：超时时间（毫秒），0 表示无限等待

例如：

```shell
SET important-key important-value
WAIT 1 5000
```

这会在写入后阻塞等待，直到至少 1 个从库确认收到该写命令，或者超过 5 秒超时。

需要注意的是，`WAIT` 只保证从库**收到了**命令，不保证从库已经**持久化**了命令。如果从库收到命令后、落盘前宕机，
数据仍然可能丢失。所以，WAIT 提供的是”尽力而为”的同步保障，而非严格的强一致性。


## 6.3 min-replicas 配置

为了防止在网络分区时主库继续接受写入（导致分区恢复后数据冲突），Redis 提供了以下配置：

```
# 至少有 N 个从库在线且延迟不超过 max-lag 秒时，主库才接受写操作
min-replicas-to-write 1
min-replicas-max-lag 10
```

如果在线从库数量不足或复制延迟过大，主库会拒绝写操作（返回错误）。这是一种**写入降级**策略，
通过牺牲可用性来保护数据一致性。


# 7 PSYNC2：故障切换后的增量同步

## 7.1 PSYNC1 的局限

在 Redis 4.0 之前，PSYNC 命令使用 **runID + offset** 来判断是否可以增量同步。这存在一个问题：
当从库被提升为新主库后，它的 runID 会改变。其他从库再连接到新主库时，发现 runID 不匹配，
只能进行全量同步，即使数据差异很小。

同样，如果一个从库重启了，它本地保存的主库 runID 和自己的 offset 都会丢失，也只能全量同步。

在大规模集群中，每次故障切换都触发所有从库全量同步，这是一个非常大的开销。


## 7.2 PSYNC2 的改进

Redis 4.0 引入了 **PSYNC2** 协议，通过两个关键改进解决了上述问题：

**1. 双 Replication ID 机制**

每个 Redis 实例维护两个复制 ID：
- `replid`：当前的复制 ID
- `replid2`：上一个主库的复制 ID（用于故障切换场景）

当从库被提升为新主库时：
- 新主库的 `replid` 设置为一个新生成的 ID
- 新主库的 `replid2` 保存为旧主库的 `replid`

其他从库连接到新主库时，新主库会检查从库发来的 `runID` 是否匹配 `replid` 或 `replid2`。
如果匹配 `replid2`（即大家曾经属于同一个主库），并且 offset 在 repl_backlog_buffer 范围内，
就可以进行增量同步，不需要全量同步。

**2. 从库重启后的增量同步**

Redis 4.0 还允许从库在关闭前将 `replid` 和 `offset` 持久化到 RDB 文件中。从库重启后，
可以从 RDB 文件中恢复这些信息，然后尝试向主库发起增量同步，而不是每次重启都全量同步。


# 8 从库的过期键处理

Redis 的键过期删除由主库控制，从库本身**不会主动删除过期键**。具体机制如下：

**主库侧**：主库通过两种方式删除过期键——惰性删除（访问时检查）和定期删除（定时扫描）。
当主库删除一个过期键时，会向所有从库发送一条 `DEL` 命令。

**从库侧**：从库收到读请求时，即使发现键已过期，在 Redis 3.2 之前也会返回该键的值（因为从库不能主动删除）。
这会导致**从库读到过期数据**的问题。

**Redis 3.2 的改进**：从 Redis 3.2 开始，从库在处理读请求时会检查键是否过期。如果已过期，从库不会返回该键的值，
而是返回空（就像键不存在一样），但从库仍然不会主动删除这个键，而是等待主库的 DEL 命令。

这个机制意味着：在从库上执行 `DBSIZE` 或 `KEYS` 等命令时，可能会统计到已过期但尚未被主库删除的键。
这在某些场景下需要特别注意。


# 9 复制相关配置汇总

```
# 设置当前实例为某个主库的从库
replicaof <masterip> <masterport>

# 主库连接密码（如果主库设置了 requirepass）
masterauth <master-password>

# 从库是否只读，默认 yes（强烈建议保持 yes）
replica-read-only yes

# 无盘复制：主库直接通过 socket 发送 RDB 数据，不先写磁盘
# 适用于磁盘慢但网络快的场景，默认 no（Redis 6.0+ 可设为 yes）
repl-diskless-sync no

# 无盘复制时，等待多少秒再开始传输（等待更多从库连接以并行传输）
repl-diskless-sync-delay 5

# 复制积压缓冲区大小，默认 1MB（建议生产环境调大到 100MB+）
repl-backlog-size 1mb

# 主库失去所有从库后，保留积压缓冲区的时间（秒），0 表示永不释放
repl-backlog-ttl 3600

# 复制超时时间，默认 60 秒
repl-timeout 60

# 从库向主库发送 REPLCONF ACK 的频率（秒）
repl-ping-replica-period 10

# 从库的 replication buffer 限制
# 格式：client-output-buffer-limit replica <hard-limit> <soft-limit> <seconds>
# 超过硬限制立即断开，超过软限制持续 seconds 秒后断开
client-output-buffer-limit replica 256mb 64mb 60

# 最少在线从库数量和最大延迟，不满足时主库拒绝写入
min-replicas-to-write 0
min-replicas-max-lag 10
```

其中几个关键参数的生产建议：
- `repl-backlog-size`：默认 1MB 在大多数生产场景下偏小，建议设置为 **100MB ~ 512MB**，视写入量和网络延迟而定
- `client-output-buffer-limit replica`：写入量大时适当调大，避免全量同步期间 buffer 溢出导致循环全量同步
- `repl-diskless-sync`：SSD 环境下设为 no（磁盘够快），HDD 环境下可以考虑设为 yes


# 10 复制状态监控

通过 `INFO replication` 命令可以查看主从复制的详细状态：

**主库上执行**：

```
# Replication
role:master
connected_slaves:2
slave0:ip=172.16.19.5,port=6379,state=online,offset=1234567,lag=0
slave1:ip=172.16.19.6,port=6379,state=online,offset=1234560,lag=1
master_replid:8371445ee590062e4c2b1b92e3028c4723a9cdd0
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:1234567
repl_backlog_active:1
repl_backlog_size:104857600
repl_backlog_first_byte_offset:1000000
repl_backlog_histlen:234567
```

**关键监控指标**：

| 指标 | 含义 | 关注点 |
|---|---|---|
| `connected_slaves` | 在线从库数量 | 下降说明有从库断连 |
| `slave0:lag` | 从库复制延迟（秒） | 持续 > 1 秒需要排查 |
| `master_repl_offset` - `slave_offset` | 主从偏移量差值 | 差值持续增大说明从库跟不上 |
| `repl_backlog_histlen` | 积压缓冲区中有效数据长度 | 接近 `repl_backlog_size` 时有覆盖风险 |

**从库上执行**时，还可以看到 `master_link_status`（与主库的连接状态）、`master_last_io_seconds_ago`（距上次与主库通信的秒数）
等字段，用于判断从库是否正常跟进复制。


# 11 总结

Redis 主从复制的三种同步模式及其适用场景：

| 同步模式 | 触发条件 | 开销 | 适用场景 |
|---|---|---|---|
| **全量复制** | 首次同步、runID 不匹配、积压缓冲区溢出 | 高（fork + RDB + 网络传输） | 不可避免的初始化同步 |
| **命令传播** | 全量复制完成后的稳态阶段 | 低（长连接逐条传播） | 正常运行期间 |
| **增量复制** | 网络断连恢复且积压缓冲区未溢出 | 中（只传差异命令） | 短暂断连恢复 |

**生产建议**：
- 实例数据量控制在几 GB 级别，减少全量同步开销
- 采用”主-从-从”级联模式分担主库压力
- `repl-backlog-size` 调大到 100MB+，减少断连后触发全量同步的概率
- 关注 `INFO replication` 中的 lag 和偏移量差值，及时发现复制延迟
- 对数据安全性要求高的场景，配合 `min-replicas-to-write` 和 `WAIT` 命令使用